"""
DeepSeekå¯¹è¯å¯¼å‡ºå·¥å…· - ç»ˆæç‰ˆ
åŠŸèƒ½ï¼šå°†DeepSeekå¯¼å‡ºçš„conversations.jsonè½¬æ¢ä¸ºMarkdownæ–‡ä»¶
ä½œè€…ï¼šDeepSeek & çƒ­å¿ƒç”¨æˆ·
ç‰ˆæœ¬ï¼š2.0
"""

import json
import os
import logging
from datetime import datetime
import re

def setup_logging(log_file='deepseek_conversion_log.txt'):
    """è®¾ç½®è¯¦ç»†çš„æ—¥å¿—é…ç½®"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    logging.info("ğŸš€ DeepSeekå¯¹è¯å¯¼å‡ºå·¥å…·å¯åŠ¨")

def sanitize_filename(title):
    """
    å®‰å…¨çš„æ–‡ä»¶åæ¸…ç†å‡½æ•°
    å¤„ç†æ‰€æœ‰éæ³•å­—ç¬¦ï¼š<>:"/\\|?*å’Œæ¢è¡Œç¬¦ç­‰
    """
    if not title or not isinstance(title, str):
        return "untitled"
    
    # ç§»é™¤æ‰€æœ‰éæ³•å­—ç¬¦
    title = re.sub(r'[<>:"/\\|?*\n\r\t]', '', title)
    # ç§»é™¤é¦–å°¾ç©ºæ ¼å’Œç‚¹å·
    title = title.strip().strip('.')
    # é™åˆ¶é•¿åº¦
    if len(title) > 50:
        title = title[:50]
    # å¦‚æœæ ‡é¢˜ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åç§°
    if not title:
        title = f"conversation_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    return title

def parse_conversation(conversation):
    """è§£æå•ä¸ªä¼šè¯çš„å®Œæ•´ä¿¡æ¯"""
    try:
        title = conversation.get('title', 'æ— æ ‡é¢˜å¯¹è¯')
        conversation_id = conversation.get('id', 'unknown')
        inserted_at = conversation.get('inserted_at', '')
        updated_at = conversation.get('updated_at', '')
        
        # è§£ææ—¶é—´
        def format_time(timestamp):
            try:
                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                return dt.strftime('%Y-%m-%d %H:%M:%S')
            except:
                return timestamp
        
        inserted_str = format_time(inserted_at)
        updated_str = format_time(updated_at)
        
        mapping = conversation.get('mapping', {})
        messages = []
        current_id = mapping.get('root', {}).get('children', [])[0] if mapping.get('root') else None
        
        # éå†æ‰€æœ‰æ¶ˆæ¯èŠ‚ç‚¹
        while current_id and current_id in mapping:
            node = mapping[current_id]
            message = node.get('message')
            
            if message:
                fragments = message.get('fragments', [])
                request_content = ""
                think_content = ""
                response_content = ""
                model = message.get('model', 'unknown')
                files = message.get('files', [])
                
                for fragment in fragments:
                    frag_type = fragment.get('type')
                    frag_content = fragment.get('content', '')
                    
                    if frag_type == 'REQUEST':
                        request_content += frag_content + "\n"
                    elif frag_type == 'THINK':
                        think_content += frag_content + "\n"
                    elif frag_type == 'RESPONSE':
                        response_content += frag_content + "\n"
                
                messages.append({
                    'model': model,
                    'files': files,
                    'request': request_content.strip(),
                    'think': think_content.strip(),
                    'response': response_content.strip(),
                    'timestamp': message.get('inserted_at', '')
                })
            
            # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæ¶ˆæ¯
            children = node.get('children', [])
            current_id = children[0] if children else None
        
        return {
            'title': title,
            'id': conversation_id,
            'inserted_at': inserted_str,
            'updated_at': updated_str,
            'message_count': len(messages),
            'messages': messages
        }
        
    except Exception as e:
        logging.error(f"è§£æä¼šè¯æ—¶å‡ºé”™: {e}")
        return None

def convert_to_markdown(conversation_data):
    """ç”Ÿæˆæ ¼å¼ä¼˜ç¾çš„Markdownå†…å®¹"""
    if not conversation_data:
        return "# è§£æå¤±è´¥\n\nè¯¥ä¼šè¯æ•°æ®æ ¼å¼å¼‚å¸¸"
    
    md_content = []
    
    # å…ƒæ•°æ®å¤´éƒ¨
    md_content.append(f"# ğŸ’¬ {conversation_data['title']}\n")
    md_content.append("## ğŸ“‹ ä¼šè¯ä¿¡æ¯\n")
    md_content.append(f"- **ğŸ—‚ï¸ ID**: `{conversation_data['id']}`")
    md_content.append(f"- **ğŸ• åˆ›å»ºæ—¶é—´**: {conversation_data['inserted_at']}")
    md_content.append(f"- **ğŸ”„ æ›´æ–°æ—¶é—´**: {conversation_data['updated_at']}")
    md_content.append(f"- **ğŸ’­ æ¶ˆæ¯æ•°é‡**: {conversation_data['message_count']} æ¡\n")
    
    md_content.append("---\n")
    
    # å¯¹è¯å†…å®¹
    for i, message in enumerate(conversation_data['messages'], 1):
        md_content.append(f"## ğŸ”„ ç¬¬ {i} è½®å¯¹è¯\n")
        
        if message['request']:
            md_content.append("### ğŸ‘¤ ç”¨æˆ·æé—®\n")
            md_content.append(f"{message['request']}\n")
        
        if message['think'] or message['response']:
            md_content.append("### ğŸ¤– DeepSeekå›å¤\n")
            md_content.append(f"**ğŸ§  æ¨¡å‹**: `{message['model']}`\n")
            
            if message['files']:
                md_content.append(f"**ğŸ“ æ–‡ä»¶**: {len(message['files'])} ä¸ªé™„ä»¶\n")
            
            if message['think']:
                md_content.append("#### ğŸ’­ æ€è€ƒè¿‡ç¨‹\n")
                md_content.append(f"{message['think']}\n")
            
            if message['response']:
                md_content.append("#### ğŸ“ å›å¤å†…å®¹\n")
                md_content.append(f"{message['response']}\n")
        
        md_content.append("---\n")
    
    # å°¾éƒ¨ä¿¡æ¯
    md_content.append(f"*å¯¼å‡ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*")
    md_content.append("*ä½¿ç”¨DeepSeekå¯¼å‡ºå·¥å…·ç”Ÿæˆ*")
    
    return "\n".join(md_content)

def process_deepseek_export(input_file="conversations.json", output_dir="DeepSeek_Conversations"):
    """ä¸»å¤„ç†å‡½æ•°"""
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è¯»å–æ•°æ®
    try:
        logging.info(f"ğŸ“– è¯»å–æ–‡ä»¶: {input_file}")
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        logging.info(f"âœ… æˆåŠŸè¯»å– {len(data)} ä¸ªä¼šè¯")
    except Exception as e:
        logging.error(f"âŒ è¯»å–å¤±è´¥: {e}")
        return
    
    # å¤„ç†ç»Ÿè®¡
    stats = {
        'total': len(data),
        'success': 0,
        'failed': 0,
        'failed_indices': []
    }
    
    # å¤„ç†æ¯ä¸ªä¼šè¯
    for i, conversation in enumerate(data, 1):
        try:
            title = conversation.get('title', f'ä¼šè¯_{i}')
            logging.info(f"ğŸ”„ å¤„ç† [{i}/{stats['total']}]: {title[:30]}...")
            
            parsed_data = parse_conversation(conversation)
            if not parsed_data:
                raise ValueError("è§£æè¿”å›ç©ºæ•°æ®")
            
            md_content = convert_to_markdown(parsed_data)
            
            # ç”Ÿæˆå®‰å…¨æ–‡ä»¶å
            safe_title = sanitize_filename(parsed_data['title'])
            filename = f"{safe_title}.md"
            filepath = os.path.join(output_dir, filename)
            
            # å¤„ç†é‡å¤æ–‡ä»¶
            counter = 1
            while os.path.exists(filepath):
                filename = f"{safe_title}_{counter}.md"
                filepath = os.path.join(output_dir, filename)
                counter += 1
            
            # ä¿å­˜æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(md_content)
            
            stats['success'] += 1
            logging.info(f"âœ… ä¿å­˜æˆåŠŸ: {filename}")
            
        except Exception as e:
            stats['failed'] += 1
            stats['failed_indices'].append(i)
            logging.error(f"âŒ å¤„ç†å¤±è´¥ [{i}]: {e}")
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_report(stats, output_dir)

def generate_report(stats, output_dir):
    """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
    report = [
        "=" * 60,
        "ğŸ“Š DeepSeekå¯¹è¯å¯¼å‡ºæŠ¥å‘Š",
        "=" * 60,
        f"æ€»ä¼šè¯æ•°: {stats['total']}",
        f"æˆåŠŸå¯¼å‡º: {stats['success']} âœ…",
        f"å¤±è´¥æ•°é‡: {stats['failed']} âŒ",
        "",
        "ğŸ“ è¾“å‡ºç›®å½•:",
        f"  {os.path.abspath(output_dir)}",
        "",
        "ğŸ• å¯¼å‡ºæ—¶é—´:",
        f"  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        ""
    ]
    
    if stats['failed_indices']:
        report.append("âŒ å¤±è´¥ä¼šè¯ç´¢å¼•:")
        for index in stats['failed_indices']:
            report.append(f"  - ç¬¬ {index} æ¡ä¼šè¯")
    
    report.append("")
    report.append("ğŸ’¡ æç¤º: å¤±è´¥é€šå¸¸æ˜¯ç”±äºæ ‡é¢˜åŒ…å«ç‰¹æ®Šå­—ç¬¦")
    report.append("=" * 60)
    
    report_text = "\n".join(report)
    print(report_text)
    
    # ä¿å­˜æŠ¥å‘Š
    with open('export_report.txt', 'w', encoding='utf-8') as f:
        f.write(report_text)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ DeepSeekå¯¹è¯å¯¼å‡ºå·¥å…· v2.0")
    print("ğŸ“ ä½œè€…: DeepSeek & çƒ­å¿ƒç”¨æˆ·")
    print("=" * 50)
    
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    
    input_file = "conversations.json"
    output_dir = "DeepSeek_Conversations"
    
    if not os.path.exists(input_file):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° {input_file}")
        print("ğŸ’¡ è¯·å°†DeepSeekå¯¼å‡ºçš„conversations.jsonæ”¾åœ¨åŒä¸€ç›®å½•")
        return
    
    print("ğŸ”„ å¼€å§‹å¤„ç†...")
    process_deepseek_export(input_file, output_dir)
    print("ğŸ‰ å¤„ç†å®Œæˆï¼æŸ¥çœ‹ export_report.txt è·å–è¯¦ç»†æŠ¥å‘Š")

if __name__ == "__main__":
    main()
