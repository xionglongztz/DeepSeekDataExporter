"""
DeepSeekå¯¹è¯å¯¼å‡ºå·¥å…· - ç»ˆæç‰ˆ
åŠŸèƒ½ï¼šå°†DeepSeekå¯¼å‡ºçš„conversations.jsonè½¬æ¢ä¸ºMarkdownæ–‡ä»¶
ä½œè€…ï¼šDeepSeek & çƒ­å¿ƒç”¨æˆ·
ç‰ˆæœ¬ï¼š2.0
"""

import json
import os
import re
from datetime import datetime
import shutil
import collections

def sanitize_filename(title):
    """æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œä½¿å…¶é€‚åˆä½œä¸ºæ–‡ä»¶å"""
    sanitized = re.sub(r'[<>:"/\\|?*]', '', title)
    if len(sanitized) > 100:
        sanitized = sanitized[:100]
    return sanitized.strip()

def parse_timestamp(timestamp_str):
    """è§£ææ—¶é—´æˆ³å­—ç¬¦ä¸²"""
    try:
        if '+' in timestamp_str:
            timestamp_str = timestamp_str.split('+')[0]
        return datetime.fromisoformat(timestamp_str)
    except:
        return datetime.now()

def count_messages(mapping):
    """è®¡ç®—å¯¹è¯ä¸­çš„æ¶ˆæ¯æ•°é‡"""
    count = 0
    for node_id, node in mapping.items():
        if node_id != "root" and node.get("message") and node["message"].get("fragments"):
            count += 1
    return count

def extract_message_content(message_node):
    """ä»æ¶ˆæ¯èŠ‚ç‚¹ä¸­æå–ç”¨æˆ·æé—®å’ŒAIå›å¤"""
    if not message_node or not message_node.get("fragments"):
        return {"user_question": "", "ai_thoughts": [], "ai_responses": []}
    
    user_question = ""
    ai_thoughts = []
    ai_responses = []
    
    for fragment in message_node["fragments"]:
        fragment_type = fragment.get("type", "")
        content = fragment.get("content", "").strip()
        
        if fragment_type == "REQUEST":
            user_question = content
        elif fragment_type == "THINK":
            if content:
                ai_thoughts.append(content)
        elif fragment_type == "RESPONSE":
            if content:
                ai_responses.append(content)
    
    return {
        "user_question": user_question,
        "ai_thoughts": ai_thoughts,
        "ai_responses": ai_responses
    }

def build_conversation_flow(mapping, log_message=None):
    """æ„å»ºå®Œæ•´çš„å¯¹è¯æµç¨‹ï¼Œä½¿ç”¨æ·±åº¦ä¼˜å…ˆéå†"""
    
    # å¦‚æœæ²¡æœ‰æä¾›æ—¥å¿—å‡½æ•°ï¼Œä½¿ç”¨é»˜è®¤çš„ç©ºå‡½æ•°
    if log_message is None:
        def log_message(msg):
            pass
    
    conversation_flow = []
    
    # æ”¶é›†æ‰€æœ‰æœ‰æ¶ˆæ¯çš„èŠ‚ç‚¹
    for node_id, node in mapping.items():
        if node_id != "root" and node.get("message") and node["message"].get("fragments"):
            message_data = extract_message_content(node["message"])
            
            has_request = any(frag.get("type") == "REQUEST" for frag in node["message"]["fragments"])
            has_response = any(frag.get("type") in ["THINK", "RESPONSE"] for frag in node["message"]["fragments"])
            
            if has_request or has_response:
                conversation_flow.append({
                    "node": node,
                    "message_data": message_data,
                    "is_user": has_request,
                    "is_ai": has_response
                })
    
    log_message(f"    - æ”¶é›†åˆ° {len(conversation_flow)} ä¸ªæœ‰æ•ˆèŠ‚ç‚¹")
    
    # å®‰å…¨çš„æ’åºé€»è¾‘
    def safe_sort_key(item):
        node_id = item['node']['id']
        log_message(f"    - æ’åºå¤„ç†èŠ‚ç‚¹ID: {node_id}, ç±»å‹: {type(node_id)}")
        
        if node_id is None:
            return (0, "")  # None å€¼æ’åœ¨å‰é¢
        
        try:
            numeric_id = int(node_id)
            return (1, numeric_id)  # æ•°å­—æ’åœ¨å‰é¢
        except (ValueError, TypeError):
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç¡®ä¿ä¸æ˜¯ None
            return (2, str(node_id) if node_id is not None else "")
    
    # å°è¯•æ’åº
    try:
        conversation_flow.sort(key=safe_sort_key)
        log_message("    - èŠ‚ç‚¹æ’åºæˆåŠŸ")
    except Exception as e:
        log_message(f"    - èŠ‚ç‚¹æ’åºå¤±è´¥: {e}")
        log_message("    - ä½¿ç”¨é»˜è®¤é¡ºåº")
        # ä¿æŒåŸé¡ºåº
    
    return conversation_flow

def generate_markdown(conversation, output_dir, log_message=None):
    """ä¸ºå•ä¸ªå¯¹è¯ç”ŸæˆMarkdownæ–‡ä»¶"""
    
    # å¦‚æœæ²¡æœ‰æä¾›æ—¥å¿—å‡½æ•°ï¼Œä½¿ç”¨é»˜è®¤çš„ç©ºå‡½æ•°
    if log_message is None:
        def log_message(msg):
            pass
    
    def create_anchor(node_id):
        """åˆ›å»ºæœ‰æ•ˆçš„Markdowné”šç‚¹"""
        anchor = re.sub(r'[^\w\-_]', '-', node_id)
        return anchor
    
    title = conversation.get("title", "æœªå‘½åå¯¹è¯")
    conversation_id = conversation.get("id", "æœªçŸ¥ID")
    inserted_at = conversation.get("inserted_at", "")
    updated_at = conversation.get("updated_at", "")
    mapping = conversation.get("mapping", {})
    
    log_message(f"  - å¼€å§‹å¤„ç†å¯¹è¯ '{title}'")
    log_message(f"  - å¯¹è¯ID: {conversation_id}")
    log_message(f"  - æ˜ å°„èŠ‚ç‚¹æ•°é‡: {len(mapping)}")
    
    # è®¡ç®—æ¶ˆæ¯æ•°é‡
    message_count = count_messages(mapping)
    log_message(f"  - æœ‰æ•ˆæ¶ˆæ¯æ•°é‡: {message_count}")
    
    # åˆ›å»ºæ–‡ä»¶å
    safe_title = sanitize_filename(title)
    filename = f"{safe_title}.md"
    filepath = os.path.join(output_dir, filename)
    
    # å¤„ç†é‡å¤æ–‡ä»¶å
    counter = 1
    original_filepath = filepath
    while os.path.exists(filepath):
        name, ext = os.path.splitext(original_filepath)
        filepath = f"{name}_{counter}{ext}"
        counter += 1
    
    # æ„å»ºå¯¹è¯æµç¨‹
    log_message(f"  - å¼€å§‹æ„å»ºå¯¹è¯æµç¨‹")
    conversation_flow = build_conversation_flow(mapping, log_message)
    log_message(f"  - å¯¹è¯æµç¨‹æ„å»ºå®Œæˆï¼Œå…± {len(conversation_flow)} ä¸ªèŠ‚ç‚¹")
    
    # ç”ŸæˆMarkdownå†…å®¹
    md_content = []
    
    # æ ‡é¢˜
    md_content.append(f"# ğŸ’¬ {title}\n")
    
    # ä¼šè¯ä¿¡æ¯
    md_content.append("## ğŸ“‹ ä¼šè¯ä¿¡æ¯\n")
    md_content.append(f"- **ğŸ—‚ï¸ ID**: `{conversation_id}`")
    md_content.append(f"- **ğŸ• åˆ›å»ºæ—¶é—´**: {parse_timestamp(inserted_at).strftime('%Y-%m-%d %H:%M:%S')}")
    md_content.append(f"- **ğŸ”„ æ›´æ–°æ—¶é—´**: {parse_timestamp(updated_at).strftime('%Y-%m-%d %H:%M:%S')}")
    md_content.append(f"- **ğŸ’­ æ¶ˆæ¯æ•°é‡**: {message_count} æ¡\n")
    
    md_content.append("---\n")
    
    # å¯¹è¯å†…å®¹
    log_message(f"  - å¼€å§‹ç”ŸæˆMarkdownå†…å®¹")
    for i, item in enumerate(conversation_flow):
        node = item["node"]
        message_data = item["message_data"]
        timestamp = parse_timestamp(node['message']['inserted_at']).strftime('%Y-%m-%d %H:%M:%S')
        model = node.get("message", {}).get("model", "æœªçŸ¥æ¨¡å‹")
        
        # ç”¨æˆ·æé—®
        if item["is_user"] and message_data["user_question"]:
            md_content.append(f"\n## ğŸ‘¤ ç”¨æˆ·")
            #md_content.append(f"\n## ğŸ‘¤ ç”¨æˆ· <a id=\"{create_anchor(node['id'])}\"></a>\n")
            md_content.append(f"{message_data['user_question']}\n")
            
            # æœç´¢ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            search_results = []
            for fragment in node.get("message", {}).get("fragments", []):
                if fragment.get("type") == "SEARCH" and fragment.get("results"):
                    search_results = fragment["results"]
                    break
    
            if search_results:
                md_content.append(f"\n**ğŸŒ ç½‘é¡µ**(å…± {len(search_results)} ä¸ª):")
                # å®‰å…¨æ’åºæœç´¢ç»“æŸ¥
                try:
                    search_results.sort(key=lambda x: x.get("cite_index", 0) or 0)
                    log_message(f"    - èŠ‚ç‚¹ {node['id']}: æˆåŠŸæ’åº {len(search_results)} ä¸ªæœç´¢ç»“æœ")
                except Exception as e:
                    log_message(f"    - èŠ‚ç‚¹ {node['id']}: æœç´¢ç»“æœæ’åºå¤±è´¥: {e}")
                    # ä¿æŒåŸé¡ºåº
        
                for result in search_results:
                    # è½¬æ¢æ—¶é—´æˆ³
                    published_at = result.get("published_at")
                    if published_at:
                        try:
                            date_str = datetime.fromtimestamp(published_at).strftime('%Y-%m-%d')
                        except:
                            date_str = "æœªçŸ¥æ—¥æœŸ"
                    else:
                        date_str = "æœªçŸ¥æ—¥æœŸ"
            
                    site_name = result.get("site_name", "æœªçŸ¥ç½‘ç«™")
                    title = result.get("title", "æ— æ ‡é¢˜")
                    url = result.get("url", "")
                    snippet = result.get("snippet", "")
            
                    md_content.append("> **ç½‘ç«™**: " + site_name + f" `{date_str}`")
                    md_content.append("> **æ ‡é¢˜**: " + title)
                    if url:
                        md_content.append("> **ç½‘å€**: `" + url + "`")
                    if snippet:
                        md_content.append("> **æ‘˜è¦**:")
                        # å¤„ç†æ‘˜è¦å†…å®¹ï¼Œç¡®ä¿æ¯è¡Œéƒ½åœ¨å¼•ç”¨å—å†…
                        snippet_lines = snippet.split('\n')
                        for line in snippet_lines:
                            if line.strip():
                                md_content.append(">> " + line)
                    md_content.append(">")  # ç©ºè¡Œåˆ†éš”
            
            # æ–‡ä»¶ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            files = node.get("message", {}).get("files", [])
            if files:
                md_content.append("\n**ğŸ“ é™„ä»¶**:")
                for file_info in files:
                    # æå–æ–‡ä»¶ä¿¡æ¯
                    file_id = file_info.get('id', 'æœªçŸ¥ID')
                    file_name = file_info.get('file_name', 'æœªçŸ¥æ–‡ä»¶å')
                    file_content = file_info.get('content', '')
        
                    # ç¬¬ä¸€å±‚å¼•ç”¨ï¼šæ–‡ä»¶ä¿¡æ¯
                    md_content.append("> ğŸ†” **æ–‡ä»¶ID**: `" + file_id + "`")
                    md_content.append("> ğŸ“„ **æ–‡ä»¶å**: `" + file_name + "`")
        
                    # å¦‚æœæœ‰æ–‡ä»¶å†…å®¹
                    if file_content:
                        md_content.append("> ğŸ“‹ **æ–‡ä»¶å†…å®¹**:")
                        # æ ¹æ®æ–‡ä»¶åç¼€ç¡®å®šä»£ç å—è¯­è¨€
                        file_extension = os.path.splitext(file_name)[1].lower().lstrip('.')
                        # å¸¸è§æ–‡ä»¶ç±»å‹çš„æ˜ å°„
                        extension_map = {
                            'py': 'python',
                            'js': 'javascript',
                            'java': 'java',
                            'cpp': 'cpp',
                            'c': 'c',
                            'html': 'html',
                            'css': 'css',
                            'json': 'json',
                            'xml': 'xml',
                            'md': 'markdown',
                            'txt': 'text',
                            'log': 'text',
                            'csv': 'csv',
                            'sql': 'sql',
                            'sh': 'bash',
                            'bat': 'batch',
                            'yml': 'yaml',
                            'yaml': 'yaml',
                            'vb': 'vb.net'
                        }
                        code_lang = extension_map.get(file_extension, 'text')
                        md_content.append(f"> ```{code_lang}")
                        # ç»Ÿä¸€æ¢è¡Œç¬¦ï¼Œé¿å…å¤šä½™ç©ºè¡Œ
                        normalized_content = file_content.replace('\r\n', '\n').replace('\r', '\n')
                        # å°†å†…å®¹æŒ‰è¡Œåˆ†å‰²å¹¶é€è¡Œæ·»åŠ åˆ°ä»£ç å—ä¸­
                        content_lines = normalized_content.split('\n')
                        for line in content_lines:
                            md_content.append("> " + line)
                        md_content.append("> ```")
            
            md_content.append(f"\n*ğŸ†” {node['id']} | ğŸ• {timestamp}*")
            
            # æ·»åŠ èŠ‚ç‚¹å…³ç³»ä¿¡æ¯
            parent_id = node.get('parent')
            children_ids = node.get('children', [])
            
            if parent_id or children_ids:
                # æ„å»ºèŠ‚ç‚¹å…³ç³»å­—ç¬¦ä¸²
                relation_parts = []
    
                # åˆ›å»ºçˆ¶èŠ‚ç‚¹é“¾æ¥ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if parent_id and parent_id != "root":
                    parent_anchor = create_anchor(parent_id)
                    relation_parts.append(f"çˆ¶èŠ‚ç‚¹: [{parent_id}](#{parent_anchor})")
                elif parent_id == "root":
                    relation_parts.append(f"çˆ¶èŠ‚ç‚¹: {parent_id}")
    
                # åˆ›å»ºå­èŠ‚ç‚¹é“¾æ¥ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if children_ids:
                    children_links = []
                    for child_id in children_ids:
                        child_anchor = create_anchor(child_id)
                        children_links.append(f"[{child_id}](#{child_anchor})")
                    relation_parts.append(f"å­èŠ‚ç‚¹: {', '.join(children_links)}")
    
                md_content.append(f"\n**ğŸ”— èŠ‚ç‚¹å…³ç³»:** { ' | '.join(relation_parts) }")
            
            md_content.append("\n---\n")
        
        # AIå›å¤éƒ¨åˆ†
        if item["is_ai"] and (message_data["ai_thoughts"] or message_data["ai_responses"]):
            md_content.append(f"\n## ğŸ¤– å›å¤")
            #md_content.append(f"\n## ğŸ¤– å›å¤ <a id=\"{create_anchor(node['id'])}\"></a>\n")
            # æœç´¢ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰- åœ¨æ€è€ƒä¹‹å‰æ˜¾ç¤ºæœç´¢ç»“æœ
            search_results = []
            for fragment in node.get("message", {}).get("fragments", []):
                if fragment.get("type") == "SEARCH" and fragment.get("results"):
                    search_results = fragment["results"]
                    break
    
            if search_results:
                md_content.append(f"\n**ğŸŒ ç½‘é¡µ**(å…± {len(search_results)} ä¸ª):")
                # å®‰å…¨æ’åºæœç´¢ç»“æœ
                try:
                    search_results.sort(key=lambda x: x.get("cite_index", 0) or 0)
                    log_message(f"    - èŠ‚ç‚¹ {node['id']}: æˆåŠŸæ’åº {len(search_results)} ä¸ªæœç´¢ç»“æœ")
                except Exception as e:
                    log_message(f"    - èŠ‚ç‚¹ {node['id']}: æœç´¢ç»“æœæ’åºå¤±è´¥: {e}")
                    # ä¿æŒåŸé¡ºåº
        
                for result in search_results:
                    # è½¬æ¢æ—¶é—´æˆ³
                    published_at = result.get("published_at")
                    if published_at:
                        try:
                            date_str = datetime.fromtimestamp(published_at).strftime('%Y-%m-%d')
                        except:
                            date_str = "æœªçŸ¥æ—¥æœŸ"
                    else:
                        date_str = "æœªçŸ¥æ—¥æœŸ"
            
                    site_name = result.get("site_name", "æœªçŸ¥ç½‘ç«™")
                    title = result.get("title", "æ— æ ‡é¢˜")
                    url = result.get("url", "")
                    snippet = result.get("snippet", "")
            
                    md_content.append("> **ç½‘ç«™**: " + site_name + f" `{date_str}`")
                    # å°†æ ‡é¢˜æ”¹ä¸ºè¶…é“¾æ¥æ ¼å¼
                    if url:
                        md_content.append("> **æ ‡é¢˜**: [" + title + "](" + url + ")")
                    else:
                        md_content.append("> **æ ‡é¢˜**: " + title)
                    if snippet:
                        md_content.append("> **æ‘˜è¦**: `" + snippet + "`")
                    md_content.append("\n")  # ç©ºè¡Œåˆ†éš”

            # å°†æ€è€ƒå†…å®¹å’Œå›å¤å†…å®¹é…å¯¹å¤„ç†
            thoughts = message_data["ai_thoughts"]
            responses = message_data["ai_responses"]

            # ç¡®ä¿æ€è€ƒå†…å®¹å’Œå›å¤å†…å®¹æ•°é‡åŒ¹é…ï¼ˆé€šå¸¸æ˜¯ä¸€å¯¹ä¸€ï¼‰
            max_pairs = max(len(thoughts), len(responses))

            for pair_index in range(max_pairs):
                # å¦‚æœæ˜¯ç¬¬äºŒä¸ªåŠä»¥åçš„å›å¤å¯¹ï¼Œæ·»åŠ åˆ†éš”çº¿
                if pair_index > 0:
                    md_content.append("\n---\n\n")

                # å½“å‰å¯¹çš„æ€è€ƒå†…å®¹
                if pair_index < len(thoughts):
                    thought = thoughts[pair_index]
                    # ä¿æŒæ€è€ƒå†…å®¹çš„å¼•ç”¨æ ¼å¼ï¼Œæ­£ç¡®å¤„ç†æ¢è¡Œ
                    thought_lines = thought.split('\n')
                    md_content.append(f"\n**ğŸ’­ æ€è€ƒ**:")
                    md_content.append("> " + thought_lines[0])
                    for line in thought_lines[1:]:
                        md_content.append("> " + line)
                    md_content.append(">\n")  # ç©ºè¡Œç»“æŸæ€è€ƒ

                # å½“å‰å¯¹çš„å›å¤å†…å®¹
                if pair_index < len(responses):
                    response = responses[pair_index]
                    md_content.append(f"{response}\n")

            model = node.get("message", {}).get("model", "æœªçŸ¥æ¨¡å‹")
            md_content.append(f"\n*ğŸ†” {node['id']} | ğŸ§  {model} | ğŸ• {timestamp}*")
            
            # æ·»åŠ èŠ‚ç‚¹å…³ç³»ä¿¡æ¯
            parent_id = node.get('parent')
            children_ids = node.get('children', [])
            
            if parent_id or children_ids:
                # æ„å»ºèŠ‚ç‚¹å…³ç³»å­—ç¬¦ä¸²
                relation_parts = []
    
                # åˆ›å»ºçˆ¶èŠ‚ç‚¹é“¾æ¥ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if parent_id and parent_id != "root":
                    parent_anchor = create_anchor(parent_id)
                    relation_parts.append(f"çˆ¶èŠ‚ç‚¹: [{parent_id}](#{parent_anchor})")
                elif parent_id == "root":
                    relation_parts.append(f"çˆ¶èŠ‚ç‚¹: {parent_id}")
    
                # åˆ›å»ºå­èŠ‚ç‚¹é“¾æ¥ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if children_ids:
                    children_links = []
                    for child_id in children_ids:
                        child_anchor = create_anchor(child_id)
                        children_links.append(f"[{child_id}](#{child_anchor})")
                    relation_parts.append(f"å­èŠ‚ç‚¹: {', '.join(children_links)}")
    
                md_content.append(f"\n**ğŸ”— èŠ‚ç‚¹å…³ç³»:** { ' | '.join(relation_parts) }")
            
            md_content.append("\n---\n")
    
    # æ–‡ä»¶å°¾éƒ¨ä¿¡æ¯
    md_content.append(f"*ğŸ“„ Markdownæ–‡ä»¶ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*")
    md_content.append("*ä½¿ç”¨DeepSeekå¯¼å‡ºå·¥å…·ç”Ÿæˆ*\n")
    
    # å†™å…¥æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write('\n'.join(md_content))
    
    # å°è¯•è®¾ç½®æ–‡ä»¶åˆ›å»ºæ—¶é—´
    try:
        creation_time = parse_timestamp(inserted_at).timestamp()
        os.utime(filepath, (creation_time, creation_time))
    except:
        pass
    
    log_message(f"  - å®Œæˆç”ŸæˆMarkdownæ–‡ä»¶: {os.path.basename(filepath)}")
    return filepath

def json_to_markdown_converter(json_file_path):
    """ä¸»è½¬æ¢å‡½æ•°"""
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "output"
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir)
    os.makedirs(output_dir)
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    log_file = f"conversion_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    log_content = []
    
    def log_message(message):
        """å°†æ¶ˆæ¯åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ—¥å¿—å†…å®¹"""
        print(message)
        log_content.append(message)
    
    log_message(f"DeepSeek JSON è½¬ Markdown è½¬æ¢æ—¥å¿—")
    log_message(f"è½¬æ¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    log_message(f"è¾“å…¥æ–‡ä»¶: {json_file_path}")
    log_message(f"è¾“å‡ºç›®å½•: {output_dir}")
    log_message("=" * 50)
    
    try:
        # è¯»å–JSONæ–‡ä»¶
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not isinstance(data, list):
            log_message("âŒ é”™è¯¯: JSONæ•°æ®åº”è¯¥æ˜¯ä¸€ä¸ªæ•°ç»„")
            return False
        
        # å¤„ç†æ¯ä¸ªå¯¹è¯
        successful_conversions = 0
        total_conversations = len(data)
        
        for i, conversation in enumerate(data):
            try:
                if isinstance(conversation, str) and conversation.startswith("...<"):
                    log_message(f"ğŸ“ æ³¨æ„: æ£€æµ‹åˆ°æˆªæ–­ä¿¡æ¯: {conversation}")
                    continue
                
                title = conversation.get("title", f"å¯¹è¯_{i+1}")
                
                # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯åˆ°æ—¥å¿—
                log_message(f"æ­£åœ¨å¤„ç†å¯¹è¯ {i+1}/{total_conversations}: {title}")
                log_message(f"å¯¹è¯ID: {conversation.get('id')}")
                
                filepath = generate_markdown(conversation, output_dir, log_message)
                
                log_message(f"âœ… æˆåŠŸè½¬æ¢: {title} -> {os.path.basename(filepath)}")
                successful_conversions += 1
                
            except Exception as e:
                # æ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                import traceback
                error_details = traceback.format_exc()
                log_message(f"âŒ è½¬æ¢å¤±è´¥ - {conversation.get('title', f'å¯¹è¯_{i+1}')}: {str(e)}")
                log_message(f"é”™è¯¯è¯¦æƒ…:")
                for line in error_details.split('\n'):
                    if line.strip():
                        log_message(f"  {line}")
        
        # ç”Ÿæˆæ€»ç»“
        log_message("=" * 50)
        log_message(f"ğŸ“Š è½¬æ¢æ€»ç»“:")
        log_message(f"   æ€»å¯¹è¯æ•°: {total_conversations}")
        log_message(f"   æˆåŠŸè½¬æ¢: {successful_conversions}")
        log_message(f"   å¤±è´¥æ•°: {total_conversations - successful_conversions}")
        log_message(f"   è¾“å‡ºç›®å½•: {os.path.abspath(output_dir)}")
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        log_message(f"âŒ è‡´å‘½é”™è¯¯: {str(e)}")
        log_message(f"é”™è¯¯å †æ ˆ:")
        for line in error_details.split('\n'):
            if line.strip():
                log_message(f"  {line}")
        return False
    
    # å†™å…¥æ—¥å¿—æ–‡ä»¶
    with open(log_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(log_content))
    
    print(f"è½¬æ¢å®Œæˆï¼")
    print(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    return True

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    json_file = "conversations.json"  # æ›¿æ¢ä¸ºä½ çš„å®é™…æ–‡ä»¶è·¯å¾„
    
    if os.path.exists(json_file):
        success = json_to_markdown_converter(json_file)
        if success:
            print("ğŸ‰ æ‰€æœ‰å¯¹è¯å·²æˆåŠŸè½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼")
        else:
            print("âŒ è½¬æ¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ã€‚")
    else:
        print(f"âŒ æ–‡ä»¶ {json_file} ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„ã€‚")
